"""
æ¨¡å‹è§£é‡Šä¸æ¯”è¾ƒæ¨¡å— - Phase 7
å®ç°æ¨¡å‹æ¯”è¾ƒã€è¯¯å·®å¯è§†åŒ–å’Œè§£é‡Šæ€§åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# é¡¹ç›®é…ç½®
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import RANDOM_SEED, TABLES_DIR, FIGURES_DIR

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.inspection import permutation_importance
from sklearn.metrics import mean_absolute_error
import shap

# è®¾ç½®éšæœºç§å­
np.random.seed(RANDOM_SEED)

# è®¾ç½®ç»˜å›¾æ ·å¼
plt.style.use('default')
sns.set_palette("husl")


class ModelInterpretation:
    """
    æ¨¡å‹è§£é‡Šä¸æ¯”è¾ƒç±»
    å®ç°æ¨¡å‹æ¯”è¾ƒã€è¯¯å·®å¯è§†åŒ–å’Œè§£é‡Šæ€§åˆ†æ
    """
    
    def __init__(self, random_seed: int = RANDOM_SEED):
        """
        åˆå§‹åŒ–æ¨¡å‹è§£é‡Šç±»
        
        Args:
            random_seed: éšæœºç§å­
        """
        self.random_seed = random_seed
        np.random.seed(random_seed)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        TABLES_DIR.mkdir(parents=True, exist_ok=True)
        FIGURES_DIR.mkdir(parents=True, exist_ok=True)
        
        # å­˜å‚¨ç»“æœ
        self.results = {}
        
    def load_data_and_predictions(self) -> Tuple[pd.DataFrame, Dict[str, np.ndarray]]:
        """
        åŠ è½½æ•°æ®å’Œé¢„æµ‹ç»“æœ
        
        Returns:
            æ•°æ®æ¡†å’Œé¢„æµ‹ç»“æœå­—å…¸
        """
        # åŠ è½½æ•°æ®
        data_path = Path(__file__).parent.parent / "data" / "processed" / "hernia_clean.csv"
        data = pd.read_csv(data_path)
        
        # åŠ è½½é¢„æµ‹ç»“æœ
        predictions = {}
        
        # åŸºçº¿æ¨¡å‹é¢„æµ‹ç»“æœ
        oof_predictions_path = TABLES_DIR / "oof_predictions.csv"
        if oof_predictions_path.exists():
            oof_df = pd.read_csv(oof_predictions_path)
            # ä½¿ç”¨predicted_valueåˆ—ä½œä¸ºé¢„æµ‹ç»“æœ
            if 'predicted_value' in oof_df.columns:
                predictions['Baseline'] = oof_df['predicted_value'].values
        
        # MLPé¢„æµ‹ç»“æœ
        mlp_oof_path = TABLES_DIR / "mlp_oof_predictions.csv"
        if mlp_oof_path.exists():
            mlp_df = pd.read_csv(mlp_oof_path)
            if 'predicted_value' in mlp_df.columns:
                predictions['MLP'] = mlp_df['predicted_value'].values
        
        # çœŸå®å€¼
        if 'duration_min' in data.columns:
            y_true = data['duration_min'].values
        else:
            # å¦‚æœæ²¡æœ‰çœŸå®å€¼ï¼Œä½¿ç”¨oof_predictionsä¸­çš„
            if oof_predictions_path.exists():
                y_true = oof_df['true_value'].values
            else:
                raise ValueError("æ— æ³•æ‰¾åˆ°çœŸå®å€¼æ•°æ®")
        
        return data, predictions, y_true
    
    def create_model_comparison_table(self, predictions: Dict[str, np.ndarray], 
                                    y_true: np.ndarray) -> pd.DataFrame:
        """
        åˆ›å»ºæ¨¡å‹æ¯”è¾ƒè¡¨
        
        Args:
            predictions: é¢„æµ‹ç»“æœå­—å…¸
            y_true: çœŸå®å€¼
            
        Returns:
            æ¨¡å‹æ¯”è¾ƒè¡¨
        """
        comparison_data = []
        
        for model_name, y_pred in predictions.items():
            if y_pred is not None and len(y_pred) == len(y_true):
                mae = mean_absolute_error(y_true, y_pred)
                rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))
                r2 = 1 - np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2)
                
                comparison_data.append({
                    'model_name': model_name,
                    'MAE': mae,
                    'RMSE': rmse,
                    'R2': r2,
                    'MAE_rank': 0,  # ç¨åå¡«å……
                    'RMSE_rank': 0,  # ç¨åå¡«å……
                    'R2_rank': 0     # ç¨åå¡«å……
                })
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if comparison_df.empty:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é¢„æµ‹ç»“æœæ•°æ®")
            return comparison_df
        
        # è®¡ç®—æ’å
        comparison_df['MAE_rank'] = comparison_df['MAE'].rank(ascending=True)
        comparison_df['RMSE_rank'] = comparison_df['RMSE'].rank(ascending=True)
        comparison_df['R2_rank'] = comparison_df['R2'].rank(ascending=False)
        
        # è®¡ç®—ç»¼åˆæ’å
        comparison_df['overall_rank'] = (comparison_df['MAE_rank'] + 
                                       comparison_df['RMSE_rank'] + 
                                       comparison_df['R2_rank']) / 3
        comparison_df['overall_rank'] = comparison_df['overall_rank'].rank()
        
        # ä¿å­˜æ¯”è¾ƒè¡¨
        comparison_path = TABLES_DIR / "model_comparison.csv"
        comparison_df.to_csv(comparison_path, index=False)
        
        print(f"æ¨¡å‹æ¯”è¾ƒè¡¨å·²ä¿å­˜åˆ°: {comparison_path}")
        return comparison_df
    
    def create_error_visualizations(self, predictions: Dict[str, np.ndarray], 
                                  y_true: np.ndarray) -> None:
        """
        åˆ›å»ºè¯¯å·®å¯è§†åŒ–å›¾è¡¨
        
        Args:
            predictions: é¢„æµ‹ç»“æœå­—å…¸
            y_true: çœŸå®å€¼
        """
        # 1. æ®‹å·®ç›´æ–¹å›¾
        self._create_residuals_histogram(predictions, y_true)
        
        # 2. å¯¹æ¯”å›¾ (y_true vs y_pred)
        self._create_parity_plot(predictions, y_true)
    
    def _create_residuals_histogram(self, predictions: Dict[str, np.ndarray], 
                                   y_true: np.ndarray) -> None:
        """
        åˆ›å»ºæ®‹å·®ç›´æ–¹å›¾
        
        Args:
            predictions: é¢„æµ‹ç»“æœå­—å…¸
            y_true: çœŸå®å€¼
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Model Residuals Distribution', fontsize=16, fontweight='bold')
        
        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„æ®‹å·®
        residuals = {}
        for model_name, y_pred in predictions.items():
            if y_pred is not None and len(y_pred) == len(y_true):
                residuals[model_name] = y_true - y_pred
        
        # ç»˜åˆ¶æ®‹å·®ç›´æ–¹å›¾
        for i, (model_name, residual) in enumerate(residuals.items()):
            row = i // 3
            col = i % 3
            
            if row < 2 and col < 3:
                axes[row, col].hist(residual, bins=50, alpha=0.7, edgecolor='black')
                axes[row, col].axvline(x=0, color='red', linestyle='--', alpha=0.8)
                axes[row, col].set_title(f'{model_name} Residuals')
                axes[row, col].set_xlabel('Residual Value')
                axes[row, col].set_ylabel('Frequency')
                axes[row, col].grid(True, alpha=0.3)
                
                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                mean_residual = np.mean(residual)
                std_residual = np.std(residual)
                axes[row, col].text(0.02, 0.98, 
                                  f'Mean: {mean_residual:.2f}\nStd: {std_residual:.2f}',
                                  transform=axes[row, col].transAxes,
                                  verticalalignment='top',
                                  bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(len(residuals), 6):
            row = i // 3
            col = i % 3
            if row < 2 and col < 3:
                axes[row, col].set_visible(False)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        residuals_path = FIGURES_DIR / "residuals_hist.png"
        plt.savefig(residuals_path, dpi=300, bbox_inches='tight')
        print(f"æ®‹å·®ç›´æ–¹å›¾å·²ä¿å­˜åˆ°: {residuals_path}")
        plt.close()
    
    def _create_parity_plot(self, predictions: Dict[str, np.ndarray], 
                           y_true: np.ndarray) -> None:
        """
        åˆ›å»ºå¯¹æ¯”å›¾ (y_true vs y_pred)
        
        Args:
            predictions: é¢„æµ‹ç»“æœå­—å…¸
            y_true: çœŸå®å€¼
        """
        n_models = len(predictions)
        if n_models == 0:
            print("è­¦å‘Š: æ²¡æœ‰é¢„æµ‹ç»“æœæ•°æ®ï¼Œè·³è¿‡å¯è§†åŒ–")
            return
        
        cols = min(3, n_models)
        rows = (n_models + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 5*rows))
        if rows == 1:
            axes = axes.reshape(1, -1)
        elif cols == 1:
            axes = axes.reshape(-1, 1)
        
        fig.suptitle('True vs Predicted Values (Parity Plot)', fontsize=16, fontweight='bold')
        
        for i, (model_name, y_pred) in enumerate(predictions.items()):
            if y_pred is not None and len(y_pred) == len(y_true):
                row = i // cols
                col = i % cols
                
                ax = axes[row, col] if rows > 1 or cols > 1 else axes[col]
                
                # ç»˜åˆ¶æ•£ç‚¹å›¾
                ax.scatter(y_true, y_pred, alpha=0.6, s=20)
                
                # ç»˜åˆ¶å¯¹è§’çº¿
                min_val = min(y_true.min(), y_pred.min())
                max_val = max(y_true.max(), y_pred.max())
                ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)
                
                ax.set_xlabel('True Values')
                ax.set_ylabel('Predicted Values')
                ax.set_title(f'{model_name}')
                ax.grid(True, alpha=0.3)
                
                # æ·»åŠ RÂ²å€¼
                r2 = 1 - np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2)
                ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}', 
                       transform=ax.transAxes, 
                       verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(n_models, rows * cols):
            row = i // cols
            col = i % cols
            if rows > 1 or cols > 1:
                axes[row, col].set_visible(False)
            else:
                axes[col].set_visible(False)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        parity_path = FIGURES_DIR / "parity_plot.png"
        plt.savefig(parity_path, dpi=300, bbox_inches='tight')
        print(f"å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {parity_path}")
        plt.close()
    
    def calculate_permutation_importance(self, data: pd.DataFrame, 
                                        predictions: Dict[str, np.ndarray],
                                        y_true: np.ndarray) -> pd.DataFrame:
        """
        è®¡ç®—æ’åˆ—é‡è¦æ€§ (ä¼˜åŒ–ç‰ˆæœ¬)
        
        Args:
            data: æ•°æ®æ¡†
            predictions: é¢„æµ‹ç»“æœå­—å…¸
            y_true: çœŸå®å€¼
            
        Returns:
            æ’åˆ—é‡è¦æ€§æ•°æ®æ¡†
        """
        print("å¼€å§‹è®¡ç®—æ’åˆ—é‡è¦æ€§...")
        
        # é€‰æ‹©æ•°å€¼ç‰¹å¾ (é™åˆ¶æ•°é‡ä»¥æé«˜é€Ÿåº¦)
        numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()
        
        # ç§»é™¤ç›®æ ‡å˜é‡å’Œä¸éœ€è¦çš„ç‰¹å¾
        exclude_features = ['duration_min', 'duration_min_log', 'unnamed:_0', 'unnamed:_0.1']
        numeric_features = [f for f in numeric_features if f not in exclude_features]
        
        # é™åˆ¶ç‰¹å¾æ•°é‡ä»¥æé«˜é€Ÿåº¦ (åªé€‰æ‹©å‰50ä¸ªæœ€é‡è¦çš„ç‰¹å¾)
        if len(numeric_features) > 50:
            print(f"ç‰¹å¾æ•°é‡è¿‡å¤š({len(numeric_features)})ï¼Œåªåˆ†æå‰50ä¸ªæœ€é‡è¦çš„ç‰¹å¾ä»¥æé«˜é€Ÿåº¦")
            # ä½¿ç”¨ç®€å•çš„ç›¸å…³æ€§é€‰æ‹©å‰50ä¸ªç‰¹å¾
            from sklearn.feature_selection import SelectKBest, f_regression
            X_temp = data[numeric_features].fillna(0)
            selector = SelectKBest(score_func=f_regression, k=50)
            selector.fit(X_temp, y_true)
            selected_features = [numeric_features[i] for i in selector.get_support(indices=True)]
            numeric_features = selected_features
        
        print(f"åˆ†æ {len(numeric_features)} ä¸ªç‰¹å¾...")
        
        # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
        X = data[numeric_features].fillna(0)
        
        importance_data = []
        
        # å¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œè®¡ç®—æ’åˆ—é‡è¦æ€§
        for model_name, y_pred in predictions.items():
            if y_pred is not None and len(y_pred) == len(y_true):
                print(f"åˆ†ææ¨¡å‹: {model_name}")
                
                # è®¡ç®—åŸºçº¿MAE
                baseline_mae = mean_absolute_error(y_true, y_pred)
                
                # ä½¿ç”¨sklearnçš„permutation_importance (æ›´å¿«)
                try:
                    from sklearn.inspection import permutation_importance
                    from sklearn.linear_model import LinearRegression
                    
                    # è®­ç»ƒä¸€ä¸ªç®€å•çš„æ¨¡å‹ç”¨äºæ’åˆ—é‡è¦æ€§è®¡ç®—
                    model = LinearRegression()
                    model.fit(X, y_true)
                    
                    # ä½¿ç”¨sklearnçš„permutation_importance
                    result = permutation_importance(
                        model, X, y_true, 
                        n_repeats=3,  # å‡å°‘é‡å¤æ¬¡æ•°
                        random_state=self.random_seed,
                        n_jobs=-1  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
                    )
                    
                    # æå–é‡è¦æ€§åˆ†æ•°
                    for i, feature in enumerate(numeric_features):
                        importance_data.append({
                            'model': model_name,
                            'feature': feature,
                            'importance': result.importances_mean[i],
                            'importance_std': result.importances_std[i],
                            'baseline_mae': baseline_mae
                        })
                    
                    print(f"âœ… {model_name} æ’åˆ—é‡è¦æ€§è®¡ç®—å®Œæˆ")
                    
                except Exception as e:
                    print(f"âš ï¸ sklearn permutation_importanceå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ³•: {e}")
                    
                    # å›é€€åˆ°ç®€åŒ–æ–¹æ³•ï¼šåªåˆ†æå‰20ä¸ªç‰¹å¾
                    top_features = numeric_features[:20]
                    print(f"ä½¿ç”¨ç®€åŒ–æ–¹æ³•åˆ†æå‰20ä¸ªç‰¹å¾...")
                    
                    for feature in top_features:
                        # ä¿å­˜åŸå§‹ç‰¹å¾å€¼
                        original_values = X[feature].copy()
                        
                        # æ‰“ä¹±ç‰¹å¾å€¼
                        X[feature] = np.random.permutation(X[feature].values)
                        
                        # é‡æ–°è®¡ç®—MAE
                        model = LinearRegression()
                        model.fit(X, y_true)
                        y_pred_permuted = model.predict(X)
                        permuted_mae = mean_absolute_error(y_true, y_pred_permuted)
                        
                        # æ¢å¤åŸå§‹ç‰¹å¾å€¼
                        X[feature] = original_values
                        
                        # è®¡ç®—é‡è¦æ€§ (MAEå¢åŠ é‡)
                        importance = permuted_mae - baseline_mae
                        
                        importance_data.append({
                            'model': model_name,
                            'feature': feature,
                            'importance': importance,
                            'importance_std': 0,  # ç®€åŒ–ç‰ˆæœ¬æ²¡æœ‰æ ‡å‡†å·®
                            'baseline_mae': baseline_mae
                        })
        
        importance_df = pd.DataFrame(importance_data)
        
        if not importance_df.empty:
            # ä¿å­˜æ’åˆ—é‡è¦æ€§ç»“æœ
            perm_importance_path = TABLES_DIR / "perm_importance.csv"
            importance_df.to_csv(perm_importance_path, index=False)
            
            # åˆ›å»ºæ’åˆ—é‡è¦æ€§å¯è§†åŒ–
            self._create_permutation_importance_plot(importance_df)
            
            print(f"æ’åˆ—é‡è¦æ€§ç»“æœå·²ä¿å­˜åˆ°: {perm_importance_path}")
        else:
            print("âš ï¸ æ²¡æœ‰ç”Ÿæˆæ’åˆ—é‡è¦æ€§æ•°æ®")
        
        return importance_df
    
    def _create_permutation_importance_plot(self, importance_df: pd.DataFrame) -> None:
        """
        åˆ›å»ºæ’åˆ—é‡è¦æ€§å¯è§†åŒ–å›¾
        
        Args:
            importance_df: æ’åˆ—é‡è¦æ€§æ•°æ®æ¡†
        """
        if importance_df.empty:
            return
        
        # é€‰æ‹©å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        if 'importance_std' in importance_df.columns:
            # æ–°ç‰ˆæœ¬ï¼šä½¿ç”¨importanceåˆ—
            top_features = importance_df.groupby('feature')['importance'].mean().abs().nlargest(20)
        else:
            # æ—§ç‰ˆæœ¬ï¼šä½¿ç”¨importanceåˆ—
            top_features = importance_df.groupby('feature')['importance'].mean().abs().nlargest(20)
        
        plt.figure(figsize=(12, 8))
        
        # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
        y_pos = np.arange(len(top_features))
        plt.barh(y_pos, top_features.values)
        plt.yticks(y_pos, top_features.index)
        plt.xlabel('Permutation Importance (Absolute Value)')
        plt.title('Top 20 Most Important Features (Permutation Importance)', fontweight='bold')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(top_features.values):
            plt.text(v + 0.001, i, f'{v:.3f}', va='center')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        perm_importance_path = FIGURES_DIR / "perm_importance.png"
        plt.savefig(perm_importance_path, dpi=300, bbox_inches='tight')
        print(f"æ’åˆ—é‡è¦æ€§å›¾å·²ä¿å­˜åˆ°: {perm_importance_path}")
        plt.close()
    
    def calculate_shap_values(self, data: pd.DataFrame, 
                            predictions: Dict[str, np.ndarray],
                            y_true: np.ndarray) -> None:
        """
        è®¡ç®—SHAPå€¼ (é€‚ç”¨äºæœ€ä¼˜æ¨¡å‹)
        
        Args:
            data: æ•°æ®æ¡†
            predictions: é¢„æµ‹ç»“æœå­—å…¸
            y_true: çœŸå®å€¼
        """
        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()
        
        # ç§»é™¤ç›®æ ‡å˜é‡å’Œä¸éœ€è¦çš„ç‰¹å¾
        exclude_features = ['duration_min', 'duration_min_log', 'unnamed:_0', 'unnamed:_0.1']
        numeric_features = [f for f in numeric_features if f not in exclude_features]
        
        # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
        X = data[numeric_features].fillna(0)
        
        # æ‰¾åˆ°æœ€ä¼˜æ¨¡å‹ (åŸºäºMAE)
        best_model = None
        best_mae = float('inf')
        
        for model_name, y_pred in predictions.items():
            if y_pred is not None and len(y_pred) == len(y_true):
                mae = mean_absolute_error(y_true, y_pred)
                if mae < best_mae:
                    best_mae = mae
                    best_model = model_name
        
        if best_model is None:
            print("æ— æ³•æ‰¾åˆ°æœ€ä¼˜æ¨¡å‹è¿›è¡ŒSHAPåˆ†æ")
            return
        
        print(f"ä½¿ç”¨æœ€ä¼˜æ¨¡å‹ {best_model} è¿›è¡ŒSHAPåˆ†æ")
        
        # ä½¿ç”¨KernelExplainer (é€‚ç”¨äºä»»ä½•æ¨¡å‹)
        try:
            # åˆ›å»ºè§£é‡Šå™¨
            explainer = shap.KernelExplainer(
                lambda x: self._predict_proxy(x, X, y_true), 
                shap.sample(X, 100)  # ä½¿ç”¨100ä¸ªæ ·æœ¬ä½œä¸ºèƒŒæ™¯
            )
            
            # è®¡ç®—SHAPå€¼
            shap_values = explainer.shap_values(shap.sample(X, 200))  # ä½¿ç”¨200ä¸ªæ ·æœ¬è®¡ç®—SHAP
            
            # åˆ›å»ºSHAPæ‘˜è¦å›¾
            plt.figure(figsize=(12, 8))
            shap.summary_plot(shap_values, X.iloc[:200], 
                            feature_names=numeric_features,
                            show=False)
            plt.title(f'SHAP Summary Plot - {best_model}', fontweight='bold')
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            shap_summary_path = FIGURES_DIR / "shap_summary.png"
            plt.savefig(shap_summary_path, dpi=300, bbox_inches='tight')
            print(f"SHAPæ‘˜è¦å›¾å·²ä¿å­˜åˆ°: {shap_summary_path}")
            plt.close()
            
        except Exception as e:
            print(f"SHAPåˆ†æå¤±è´¥: {e}")
            print("å°è¯•ä½¿ç”¨ç®€åŒ–çš„ç‰¹å¾é‡è¦æ€§åˆ†æ...")
            self._create_simple_feature_importance(data, numeric_features, y_true)
    
    def _predict_proxy(self, x: np.ndarray, X: pd.DataFrame, y_true: np.ndarray) -> np.ndarray:
        """
        SHAPçš„ä»£ç†é¢„æµ‹å‡½æ•°
        
        Args:
            x: è¾“å…¥ç‰¹å¾
            X: åŸå§‹ç‰¹å¾çŸ©é˜µ
            y_true: çœŸå®å€¼
            
        Returns:
            é¢„æµ‹å€¼
        """
        # ä½¿ç”¨ç®€å•çš„çº¿æ€§å›å½’ä½œä¸ºä»£ç†
        from sklearn.linear_model import LinearRegression
        model = LinearRegression()
        model.fit(X, y_true)
        return model.predict(x)
    
    def _create_simple_feature_importance(self, data: pd.DataFrame, 
                                        numeric_features: List[str],
                                        y_true: np.ndarray) -> None:
        """
        åˆ›å»ºç®€åŒ–çš„ç‰¹å¾é‡è¦æ€§å›¾
        
        Args:
            data: æ•°æ®æ¡†
            numeric_features: æ•°å€¼ç‰¹å¾åˆ—è¡¨
            y_true: çœŸå®å€¼
        """
        # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
        X = data[numeric_features].fillna(0)
        
        # ä½¿ç”¨çº¿æ€§å›å½’è®¡ç®—ç‰¹å¾é‡è¦æ€§
        from sklearn.linear_model import LinearRegression
        from sklearn.preprocessing import StandardScaler
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # è®­ç»ƒçº¿æ€§å›å½’
        model = LinearRegression()
        model.fit(X_scaled, y_true)
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§ (ç³»æ•°çš„ç»å¯¹å€¼)
        importance = np.abs(model.coef_)
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§æ•°æ®æ¡†
        importance_df = pd.DataFrame({
            'feature': numeric_features,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        # ç»˜åˆ¶å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        top_features = importance_df.head(20)
        
        plt.figure(figsize=(12, 8))
        y_pos = np.arange(len(top_features))
        plt.barh(y_pos, top_features['importance'])
        plt.yticks(y_pos, top_features['feature'])
        plt.xlabel('Feature Importance (Absolute Coefficient)')
        plt.title('Top 20 Most Important Features (Linear Regression)', fontweight='bold')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(top_features['importance']):
            plt.text(v + 0.001, i, f'{v:.3f}', va='center')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        shap_summary_path = FIGURES_DIR / "shap_summary.png"
        plt.savefig(shap_summary_path, dpi=300, bbox_inches='tight')
        print(f"ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜åˆ°: {shap_summary_path}")
        plt.close()
    
    def analyze_group_fairness(self, data: pd.DataFrame, 
                             predictions: Dict[str, np.ndarray],
                             y_true: np.ndarray) -> pd.DataFrame:
        """
        åˆ†æåˆ†ç»„å…¬å¹³æ€§
        
        Args:
            data: æ•°æ®æ¡†
            predictions: é¢„æµ‹ç»“æœå­—å…¸
            y_true: çœŸå®å€¼
            
        Returns:
            åˆ†ç»„MAEåˆ†æç»“æœ
        """
        group_mae_data = []
        
        # å®šä¹‰åˆ†ç»„å˜é‡
        group_vars = {
            'procedure_type': self._extract_procedure_type(data),
            'gender': self._extract_gender(data),
            'age_group': self._extract_age_group(data),
            'weekday': self._extract_weekday(data)
        }
        
        # å¯¹æ¯ä¸ªæ¨¡å‹å’Œæ¯ä¸ªåˆ†ç»„è®¡ç®—MAE
        for model_name, y_pred in predictions.items():
            if y_pred is not None and len(y_pred) == len(y_true):
                for group_name, group_values in group_vars.items():
                    if group_values is not None:
                        for group_value in group_values.unique():
                            if pd.notna(group_value):
                                # æ‰¾åˆ°å±äºè¯¥ç»„çš„æ ·æœ¬
                                mask = group_values == group_value
                                if mask.sum() > 10:  # è‡³å°‘éœ€è¦10ä¸ªæ ·æœ¬
                                    group_y_true = y_true[mask]
                                    group_y_pred = y_pred[mask]
                                    
                                    mae = mean_absolute_error(group_y_true, group_y_pred)
                                    rmse = np.sqrt(np.mean((group_y_true - group_y_pred) ** 2))
                                    count = mask.sum()
                                    
                                    group_mae_data.append({
                                        'model': model_name,
                                        'group_variable': group_name,
                                        'group_value': str(group_value),
                                        'MAE': mae,
                                        'RMSE': rmse,
                                        'sample_count': count
                                    })
        
        group_mae_df = pd.DataFrame(group_mae_data)
        
        # ä¿å­˜åˆ†ç»„MAEç»“æœ
        group_mae_path = TABLES_DIR / "group_mae.csv"
        group_mae_df.to_csv(group_mae_path, index=False)
        
        # åˆ›å»ºåˆ†ç»„MAEå¯è§†åŒ–
        self._create_group_mae_visualization(group_mae_df)
        
        print(f"åˆ†ç»„MAEåˆ†æç»“æœå·²ä¿å­˜åˆ°: {group_mae_path}")
        return group_mae_df
    
    def _extract_procedure_type(self, data: pd.DataFrame) -> pd.Series:
        """æå–æ‰‹æœ¯ç±»å‹"""
        # æŸ¥æ‰¾åŒ…å«procedureæˆ–surgeryçš„åˆ—
        proc_cols = [col for col in data.columns if 'procedure' in col.lower() or 'surgery' in col.lower()]
        if proc_cols:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„åˆ—
            return data[proc_cols[0]]
        return None
    
    def _extract_gender(self, data: pd.DataFrame) -> pd.Series:
        """æå–æ€§åˆ«ä¿¡æ¯"""
        gender_cols = [col for col in data.columns if 'sex' in col.lower() or 'gender' in col.lower()]
        if gender_cols:
            return data[gender_cols[0]]
        return None
    
    def _extract_age_group(self, data: pd.DataFrame) -> pd.Series:
        """æå–å¹´é¾„ç»„ä¿¡æ¯"""
        age_cols = [col for col in data.columns if 'age' in col.lower()]
        if age_cols:
            age = data[age_cols[0]]
            # åˆ›å»ºå¹´é¾„ç»„
            if age.dtype in ['int64', 'float64']:
                age_groups = pd.cut(age, bins=[0, 30, 50, 70, 100], 
                                  labels=['0-30', '31-50', '51-70', '70+'])
                return age_groups
        return None
    
    def _extract_weekday(self, data: pd.DataFrame) -> pd.Series:
        """æå–æ˜ŸæœŸä¿¡æ¯"""
        date_cols = [col for col in data.columns if 'date' in col.lower() or 'dttm' in col.lower()]
        if date_cols:
            # å°è¯•è§£ææ—¥æœŸåˆ—
            for col in date_cols:
                try:
                    dates = pd.to_datetime(data[col], errors='coerce')
                    weekdays = dates.dt.day_name()
                    return weekdays
                except:
                    continue
        return None
    
    def _create_group_mae_visualization(self, group_mae_df: pd.DataFrame) -> None:
        """
        åˆ›å»ºåˆ†ç»„MAEå¯è§†åŒ–å›¾
        
        Args:
            group_mae_df: åˆ†ç»„MAEæ•°æ®æ¡†
        """
        if group_mae_df.empty:
            return
        
        # åˆ›å»ºåˆ†ç»„MAEæ¡å½¢å›¾
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Group Fairness Analysis - MAE by Different Groups', fontsize=16, fontweight='bold')
        
        group_vars = group_mae_df['group_variable'].unique()
        
        for i, group_var in enumerate(group_vars):
            row = i // 2
            col = i % 2
            
            group_data = group_mae_df[group_mae_df['group_variable'] == group_var]
            
            if not group_data.empty:
                # æŒ‰æ¨¡å‹åˆ†ç»„ç»˜åˆ¶
                models = group_data['model'].unique()
                x = np.arange(len(group_data['group_value'].unique()))
                width = 0.8 / len(models)
                
                for j, model in enumerate(models):
                    model_data = group_data[group_data['model'] == model]
                    model_data = model_data.sort_values('group_value')
                    
                    axes[row, col].bar(x + j * width, model_data['MAE'], 
                                     width, label=model, alpha=0.8)
                
                axes[row, col].set_xlabel('Group Value')
                axes[row, col].set_ylabel('MAE')
                axes[row, col].set_title(f'{group_var.replace("_", " ").title()}')
                axes[row, col].set_xticks(x + width * (len(models) - 1) / 2)
                axes[row, col].set_xticklabels(group_data['group_value'].unique(), rotation=45)
                axes[row, col].legend()
                axes[row, col].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        group_mae_path = FIGURES_DIR / "group_mae_bar.png"
        plt.savefig(group_mae_path, dpi=300, bbox_inches='tight')
        print(f"åˆ†ç»„MAEæ¡å½¢å›¾å·²ä¿å­˜åˆ°: {group_mae_path}")
        plt.close()
    
    def run_complete_analysis(self) -> None:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        """
        print("å¼€å§‹è¿è¡Œæ¨¡å‹è§£é‡Šä¸æ¯”è¾ƒåˆ†æ...")
        
        try:
            # 1. åŠ è½½æ•°æ®å’Œé¢„æµ‹ç»“æœ
            print("1. åŠ è½½æ•°æ®å’Œé¢„æµ‹ç»“æœ...")
            data, predictions, y_true = self.load_data_and_predictions()
            
            # 2. åˆ›å»ºæ¨¡å‹æ¯”è¾ƒè¡¨
            print("2. åˆ›å»ºæ¨¡å‹æ¯”è¾ƒè¡¨...")
            comparison_df = self.create_model_comparison_table(predictions, y_true)
            print(comparison_df)
            
            # 3. åˆ›å»ºè¯¯å·®å¯è§†åŒ–
            print("3. åˆ›å»ºè¯¯å·®å¯è§†åŒ–...")
            self.create_error_visualizations(predictions, y_true)
            
            # 4. è®¡ç®—æ’åˆ—é‡è¦æ€§
            print("4. è®¡ç®—æ’åˆ—é‡è¦æ€§...")
            perm_importance_df = self.calculate_permutation_importance(data, predictions, y_true)
            
            # 5. è®¡ç®—SHAPå€¼
            print("5. è®¡ç®—SHAPå€¼...")
            self.calculate_shap_values(data, predictions, y_true)
            
            # 6. åˆ†æåˆ†ç»„å…¬å¹³æ€§
            print("6. åˆ†æåˆ†ç»„å…¬å¹³æ€§...")
            group_mae_df = self.analyze_group_fairness(data, predictions, y_true)
            
            print("\nâœ… æ¨¡å‹è§£é‡Šä¸æ¯”è¾ƒåˆ†æå®Œæˆï¼")
            print(f"ğŸ“Š ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {TABLES_DIR}")
            print(f"ğŸ“ˆ å›¾è¡¨æ–‡ä»¶ä¿å­˜åœ¨: {FIGURES_DIR}")
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    interpreter = ModelInterpretation()
    interpreter.run_complete_analysis()


if __name__ == "__main__":
    main()
